{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captura de Imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saliendo...\n",
      "Saliendo...\n",
      "Captura de imágenes completada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def capture_images(folder_name, image_count):\n",
    "    \"\"\"\n",
    "    Captura imágenes desde la cámara y las guarda en la carpeta especificada.\n",
    "    \n",
    "    Parámetros:\n",
    "        folder_name (str): Nombre de la carpeta donde se guardarán las imágenes.\n",
    "        image_count (int): Número inicial para nombrar las imágenes.\n",
    "    \n",
    "    Retorna:\n",
    "        int: Número total de imágenes capturadas.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara.\")\n",
    "        return image_count\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "    taking_photos = False\n",
    "    cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Frame', 640, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: No se pudo capturar el frame.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('e'):  # Presiona 'e' para comenzar a tomar fotos\n",
    "            taking_photos = True\n",
    "            start_time = time.time()\n",
    "            print(\"Comenzando a tomar fotos...\")\n",
    "\n",
    "        if key == ord('r'):  # Presiona 'r' para detener la toma de fotos\n",
    "            taking_photos = False\n",
    "            print(\"Toma de fotos detenida.\")\n",
    "\n",
    "        if taking_photos and time.time() - start_time >= 2:  # Tomar 10 fotos cada 2 segundos\n",
    "            for _ in range(10):\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    image_path = os.path.join(folder_name, f\"image_{image_count}.jpg\")\n",
    "                    cv2.imwrite(image_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                    print(f\"Imagen guardada: {image_path}\")\n",
    "                    image_count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "        if key == ord('q'):  # Presiona 'q' para salir\n",
    "            print(\"Saliendo...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return image_count\n",
    "\n",
    "# Capturar imágenes de entrenamiento\n",
    "object_name = input(\"Introduce el nombre del objeto (entrenamiento): \")\n",
    "image_count = len(os.listdir(object_name)) if os.path.exists(object_name) else 0\n",
    "image_count = capture_images(object_name, image_count)\n",
    "\n",
    "# Capturar imágenes de prueba\n",
    "test_folder = input(\"Introduce el nombre de la carpeta de prueba: \")\n",
    "image_count = len(os.listdir(test_folder)) if os.path.exists(test_folder) else 0\n",
    "image_count = capture_images(test_folder, image_count)\n",
    "\n",
    "print(\"Captura de imágenes completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(img_path)  # Leer la imagen con OpenCV\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo leer la imagen: {img_path}\")\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertir a RGB\n",
    "\n",
    "        # Recortar una región central de la imagen (por ejemplo, 50x50 píxeles)\n",
    "        h, w, _ = image.shape\n",
    "        center_x, center_y = w // 2, h // 2\n",
    "        crop_size = 50  # Tamaño de la región central\n",
    "        center_crop = image[center_y - crop_size // 2:center_y + crop_size // 2,\n",
    "                            center_x - crop_size // 2:center_x + crop_size // 2]\n",
    "\n",
    "        # Convertir la región central a espacio de color HSV\n",
    "        hsv_crop = cv2.cvtColor(center_crop, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "        # Calcular el color promedio en la región central\n",
    "        avg_color = np.mean(hsv_crop, axis=(0, 1))\n",
    "\n",
    "        # Definir un rango de colores basado en el color promedio\n",
    "        lower_color = np.array([avg_color[0] - 10, 50, 50])  # Ajusta estos valores según sea necesario\n",
    "        upper_color = np.array([avg_color[0] + 10, 255, 255])\n",
    "\n",
    "        # Convertir la imagen completa a espacio de color HSV\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "        # Crear la máscara binaria\n",
    "        mask = cv2.inRange(hsv_image, lower_color, upper_color)\n",
    "\n",
    "        # Convertir la imagen y la máscara a tensores\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.fromarray(image))  # Convertir a PIL y aplicar transformaciones\n",
    "            mask = self.transform(Image.fromarray(mask))  # Convertir a PIL y aplicar transformaciones\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definir las transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),  # Convertir a tensor\n",
    "])\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_images_folder = object_name  # Carpeta de entrenamiento\n",
    "train_dataset = CustomDataset(train_images_folder, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_images_folder = test_folder  # Carpeta de prueba\n",
    "test_dataset = CustomDataset(test_images_folder, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo de segmentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = self.conv_block(in_channels, 64)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.up1 = self.conv_block(128, 64)\n",
    "        self.up2 = self.conv_block(64, out_channels)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implementar la arquitectura U-Net completa aquí\n",
    "        pass  # Esto es un esquema simplificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLas máscaras no se cargaron correctamente.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_Proyecto_Grado\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_Proyecto_Grado\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:713\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_Proyecto_Grado\\env\\lib\\site-packages\\torch\\nn\\functional.py:2957\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2955\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 2957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[0;32m   2958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m   2960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Crear el modelo\n",
    "model = UNet(in_channels=3, out_channels=1)  # 1 canal de salida para la máscara binaria\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.BCEWithLogitsLoss()  # Pérdida para segmentación binaria\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Asegurarse de que las máscaras tengan la misma forma que las salidas\n",
    "        masks = masks.float()  # Convertir a float\n",
    "        masks = masks.unsqueeze(1)  # Añadir dimensión de canal\n",
    "        \n",
    "        # Verificar que las máscaras no sean None\n",
    "        if masks is None:\n",
    "            raise ValueError(\"Las máscaras no se cargaron correctamente.\")\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "torch.save(model.state_dict(), 'unet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recorte de imagenes usando una mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model.load_state_dict(torch.load('unet_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Función para recortar el objeto usando la máscara\n",
    "def crop_with_mask(image_path, output_folder):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_tensor = transform(Image.open(image_path).convert('RGB')).unsqueeze(0)\n",
    "    \n",
    "    # Generar la máscara\n",
    "    with torch.no_grad():\n",
    "        mask = torch.sigmoid(model(image_tensor)).squeeze().numpy()\n",
    "    \n",
    "    # Aplicar la máscara a la imagen\n",
    "    mask = (mask > 0.5).astype(np.uint8)  # Convertir a máscara binaria\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    # Guardar la imagen recortada\n",
    "    output_path = f\"{output_folder}/{os.path.basename(image_path)}\"\n",
    "    cv2.imwrite(output_path, masked_image)\n",
    "\n",
    "# Crear carpeta para imágenes recortadas\n",
    "output_folder = 'recortes'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Recortar objetos en las imágenes de prueba\n",
    "for image_file in os.listdir(test_images_folder):\n",
    "    image_path = os.path.join(test_images_folder, image_file)\n",
    "    crop_with_mask(image_path, output_folder)\n",
    "\n",
    "print(\"Recorte de objetos completado. Imágenes guardadas en la carpeta 'recortes'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
